# PPO logging üß≠

| Category                        | Metric                                           | Meaning                                            | Healthy Range / Watch For     |
| ------------------------------- | ------------------------------------------------ | -------------------------------------------------- | ----------------------------- |
| üéØ **Reward**                   | `env/reward_mean`                                | Avg. reward (performance)                          | ‚Üë steadily over training      |
|                                 | `env/reward_std`                                 | Reward variability                                 | Moderate; not exploding       |
|                                 | `env/reward_dist`                                | Reward distribution                                | Balanced; no long tails       |
| üîÄ **KL Divergence**            | `objective/kl`                                   | Divergence old‚Üînew policy                          | Small positive (‚âà 0.01‚Äì0.1)   |
|                                 | `objective/kl_coef`                              | KL penalty coefficient                             | Increases if KL too high      |
|                                 | `ppo/mean_non_score_reward`                      | Mean KL penalty (‚ÄìKL loss)                         | Should not dominate rewards   |
|                                 | **Watch:**                                       | If KL ‚Üë ‚Üí over-optimization; KL ‚Üì ‚Üí under-training |                               |
| üé≤ **Entropy**                  | `objective/entropy`                              | Policy randomness                                  | High early ‚Üí ‚Üì gradually      |
|                                 | `ppo/policy/entropy`                             | Same (softmax-based)                               | Tracks exploration level      |
| ‚öñÔ∏è **Policy Stability**         | `ppo/policy/clipfrac`                            | Fraction of clipped ratios                         | < 0.2 ideally                 |
|                                 | `ppo/policy/approxkl`                            | Approx. KL (fast estimator)                        | < 0.01‚Äì0.05                   |
|                                 | `ppo/policy/ratio`                               | œÄ_new / œÄ_old                                      | ‚âà 1; >200 = unstable          |
|                                 | `ppo/policy/advantages_mean`                     | Avg. GAE advantage                                 | Centered near 0               |
|                                 | `ppo/policy/advantages`                          | Advantage distribution                             | Balanced (not skewed)         |
| üìà **Value Function**           | `ppo/val/mean`, `ppo/val/var`                    | Predicted values                                   | Stable variance               |
|                                 | `ppo/val/error`                                  | Value MSE vs. returns                              | Should ‚Üì smoothly             |
|                                 | `ppo/val/var_explained`                          | % variance explained by critic                     | > 0.8 = good                  |
|                                 | `ppo/val/clipfrac`                               | Clipped value updates                              | Low (too high ‚Üí overfit)      |
|                                 | `ppo/returns/mean`, `ppo/returns/var`            | TD(Œª) returns                                      | Check scaling consistency     |
| ‚öôÔ∏è **Losses**                   | `ppo/loss/policy`                                | Actor loss                                         | Stable, small oscillations ok |
|                                 | `ppo/loss/value`                                 | Critic loss                                        | No NaNs or big spikes         |
|                                 | `ppo/loss/total`                                 | Combined objective                                 | Gradual ‚Üì trend               |
| ‚úèÔ∏è **Token Stats** *(for RLHF)* | `tokens/queries_len_mean` / `responses_len_mean` | Avg. token length                                  | Monitor for drift             |
|                                 | `tokens/responses_len_std`                       | Variability                                        | Not exploding                 |
| üìä **Log-Probs**                | `objective/logprobs`, `objective/ref_logprobs`   | Log-probs of actions vs. ref                       | Inspect for shifts            |

# Crucial Values to Remember üö®

| Metric                | Ideal Behavior             | Problem If                |
| --------------------- | -------------------------- | ------------------------- |
| `objective/kl`        | Small positive (~0.01‚Äì0.1) | Too high ‚Üí instability    |
| `ppo/policy/ratio`    | ‚âà 1                        | > 200 ‚Üí over-optimization |
| `ppo/policy/clipfrac` | < 0.2                      | High ‚Üí aggressive updates |
| `ppo/loss/value`      | Stable                     | NaN/spikes ‚Üí bad critic   |
| `env/reward_mean`     | Increasing                 | Plateau ‚Üí stagnation      |

# Core RL Concepts üß†

| Term                                       | Definition                                                                                 |
| ------------------------------------------ | ------------------------------------------------------------------------------------------ |
| **Policy (œÄ)**                             | Function mapping states to action probabilities.                                           |
| **Value Function (V(s))**                  | Expected cumulative reward from a state under œÄ.                                           |
| **Advantage (A(s,a))**                     | How much better action *a* is than average at state *s*.                                   |
| **Return (G‚Çú)**                            | Discounted sum of future rewards: Œ£ Œ≥‚Åø r‚Çú‚Çä‚Çô                                                |
| **Entropy**                                | Randomness in policy ‚Äî promotes exploration.                                               |
| **KL Divergence**                          | Measures how much new policy diverges from old policy.                                     |
| **Clipping (PPO)**                         | Restricts how much the policy ratio can deviate from 1, to avoid destructive updates.      |
| **GAE (Generalized Advantage Estimation)** | Smoothed advantage estimator balancing bias‚Äìvariance.                                      |
| **Explained Variance**                     | Fraction of variance in returns explained by the critic ‚Äî measures value function quality. |

üß© TL;DR ‚Äî What to Remember

Reward mean ‚Üë ‚Üí Learning works.

KL small & positive ‚Üí Stable policy updates.

Entropy ‚Üì slowly ‚Üí Controlled exploration to exploitation.

Clipfrac < 0.2, ratio ‚âà 1 ‚Üí PPO updates stable.

Value loss steady, explained variance high ‚Üí good critic.
