# RL Metrics

| Category                        | Metric                                           | Meaning                                            | Healthy Range / Watch For     |
| ------------------------------- | ------------------------------------------------ | -------------------------------------------------- | ----------------------------- |
| üéØ **Reward**                   | `env/reward_mean`                                | Avg. reward (performance)                          | ‚Üë steadily over training      |
|                                 | `env/reward_std`                                 | Reward variability                                 | Moderate; not exploding       |
|                                 | `env/reward_dist`                                | Reward distribution                                | Balanced; no long tails       |
| üîÄ **KL Divergence**            | `objective/kl`                                   | Divergence old‚Üînew policy                          | Small positive (‚âà 0.01‚Äì0.1)   |
|                                 | `objective/kl_coef`                              | KL penalty coefficient                             | Increases if KL too high      |
|                                 | `ppo/mean_non_score_reward`                      | Mean KL penalty (‚ÄìKL loss)                         | Should not dominate rewards   |
|                                 | **Watch:**                                       | If KL ‚Üë ‚Üí over-optimization; KL ‚Üì ‚Üí under-training |                               |
| üé≤ **Entropy**                  | `objective/entropy`                              | Policy randomness                                  | High early ‚Üí ‚Üì gradually      |
|                                 | `ppo/policy/entropy`                             | Same (softmax-based)                               | Tracks exploration level      |
| ‚öñÔ∏è **Policy Stability**         | `ppo/policy/clipfrac`                            | Fraction of clipped ratios                         | < 0.2 ideally                 |
|                                 | `ppo/policy/approxkl`                            | Approx. KL (fast estimator)                        | < 0.01‚Äì0.05                   |
|                                 | `ppo/policy/ratio`                               | œÄ_new / œÄ_old                                      | ‚âà 1; >200 = unstable          |
|                                 | `ppo/policy/advantages_mean`                     | Avg. GAE advantage                                 | Centered near 0               |
|                                 | `ppo/policy/advantages`                          | Advantage distribution                             | Balanced (not skewed)         |
| üìà **Value Function**           | `ppo/val/mean`, `ppo/val/var`                    | Predicted values                                   | Stable variance               |
|                                 | `ppo/val/error`                                  | Value MSE vs. returns                              | Should ‚Üì smoothly             |
|                                 | `ppo/val/var_explained`                          | % variance explained by critic                     | > 0.8 = good                  |
|                                 | `ppo/val/clipfrac`                               | Clipped value updates                              | Low (too high ‚Üí overfit)      |
|                                 | `ppo/returns/mean`, `ppo/returns/var`            | TD(Œª) returns                                      | Check scaling consistency     |
| ‚öôÔ∏è **Losses**                   | `ppo/loss/policy`                                | Actor loss                                         | Stable, small oscillations ok |
|                                 | `ppo/loss/value`                                 | Critic loss                                        | No NaNs or big spikes         |
|                                 | `ppo/loss/total`                                 | Combined objective                                 | Gradual ‚Üì trend               |
| ‚úèÔ∏è **Token Stats** *(for RLHF)* | `tokens/queries_len_mean` / `responses_len_mean` | Avg. token length                                  | Monitor for drift             |
|                                 | `tokens/responses_len_std`                       | Variability                                        | Not exploding                 |
| üìä **Log-Probs**                | `objective/logprobs`, `objective/ref_logprobs`   | Log-probs of actions vs. ref                       | Inspect for shifts            |


| Term                                       | Definition                                                                                 |
| ------------------------------------------ | ------------------------------------------------------------------------------------------ |
| **Policy (œÄ)**                             | Function mapping states to action probabilities.                                           |
| **Value Function (V(s))**                  | Expected cumulative reward from a state under œÄ.                                           |
| **Advantage (A(s,a))**                     | How much better action *a* is than average at state *s*.                                   |
| **Return (G‚Çú)**                            | Discounted sum of future rewards: Œ£ Œ≥‚Åø r‚Çú‚Çä‚Çô                                                |
| **Entropy**                                | Randomness in policy ‚Äî promotes exploration.                                               |
| **KL Divergence**                          | Measures how much new policy diverges from old policy.                                     |
| **Clipping (PPO)**                         | Restricts how much the policy ratio can deviate from 1, to avoid destructive updates.      |
| **GAE (Generalized Advantage Estimation)** | Smoothed advantage estimator balancing bias‚Äìvariance.                                      |
| **Explained Variance**                     | Fraction of variance in returns explained by the critic ‚Äî measures value function quality. |
