{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [],
      "dockerImageVersionId": 30747,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "name": "Reinforcement-Learning",
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nDI5hGGeXURc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hf_tokens = \"TOKEN\""
      ],
      "metadata": {
        "id": "7lXAL0_1XURd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-08-14T02:55:56.324183Z",
          "iopub.execute_input": "2024-08-14T02:55:56.324811Z",
          "iopub.status.idle": "2024-08-14T02:55:56.664825Z",
          "shell.execute_reply.started": "2024-08-14T02:55:56.324777Z",
          "shell.execute_reply": "2024-08-14T02:55:56.663935Z"
        },
        "trusted": true,
        "id": "7njkz4YFXURf",
        "outputId": "8e038d75-514a-4798-c38a-271d3ff31662",
        "colab": {
          "referenced_widgets": [
            "624d2de791fc4c36ab1f32abbbdeda78"
          ]
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "624d2de791fc4c36ab1f32abbbdeda78"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Adapter (LORA implementation) - https://towardsdatascience.com/implementing-lora-from-scratch-20f838b046f1\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "SZqrtu1OXURh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Finetune with PPO to output only positive movie review\n",
        "\n",
        "hf link - https://huggingface.co/docs/trl/en/quickstart\n",
        "\n",
        "https://github.com/hkproj/rlhf-ppo/blob/main/gpt_sentiment.py\n",
        "\n",
        "\n",
        "https://www.philschmid.de/fsdp-qlora-llama3\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "### PPO logging information\n",
        "---\n",
        "\n",
        "## **Key Metrics to Monitor**\n",
        "\n",
        "### **1. Reward Metrics**\n",
        "- **env/reward_mean**: Average reward obtained from the environment. Alias `ppo/mean_scores`, used to monitor the reward model.\n",
        "- **env/reward_std**: Standard deviation of the reward. Alias `ppo/std_scores`, used to monitor reward variability.\n",
        "- **env/reward_dist**: Histogram distribution of rewards.\n",
        "\n",
        "### **2. KL Divergence Metrics**\n",
        "- **objective/kl**: Mean Kullback-Leibler (KL) divergence between old and new policies. Measures deviation of new policy from old policy.\n",
        "- **objective/kl_dist**: Histogram distribution of KL divergence.\n",
        "- **objective/kl_coef**: Coefficient for KL divergence in the objective function.\n",
        "- **ppo/mean_non_score_reward**: KL penalty calculated as `objective/kl * objective/kl_coef`. Prevents new policy from deviating too far from old policy.\n",
        "\n",
        "### **3. Entropy Metrics**\n",
        "- **objective/entropy**: Entropy of the model’s policy (calculated as -logprobs.sum(-1).mean()). High entropy indicates more randomness in actions, beneficial for exploration.\n",
        "- **ppo/policy/entropy**: Entropy of the policy, calculated by `torch.nn.functional.softmax(logits, dim=-1)` and related operations.\n",
        "\n",
        "### **4. Policy Metrics**\n",
        "- **ppo/policy/clipfrac**: Fraction of probability ratios falling outside the clipping range in the PPO objective.\n",
        "- **ppo/policy/approxkl**: Approximate KL divergence between old and new policies (k2 estimator).\n",
        "- **ppo/policy/policykl**: KL divergence measured by masked mean (k1 estimator).\n",
        "- **ppo/policy/ratio**: Histogram distribution of the ratio between new and old policies.\n",
        "- **ppo/policy/advantages_mean**: Average of the Generalized Advantage Estimation (GAE) advantage estimates.\n",
        "- **ppo/policy/advantages**: Histogram distribution of GAE advantages.\n",
        "\n",
        "### **5. Value Function Metrics**\n",
        "- **ppo/returns/mean**: Mean of TD(λ) returns, calculated as `advantage + values`.\n",
        "- **ppo/returns/var**: Variance of TD(λ) returns.\n",
        "- **ppo/val/mean**: Mean of the value function.\n",
        "- **ppo/val/var**: Variance of the value function.\n",
        "- **ppo/val/var_explained**: Explained variance for the value function.\n",
        "- **ppo/val/clipfrac**: Fraction of value function’s predicted values that are clipped.\n",
        "- **ppo/val/vpred**: Predicted values from the value function.\n",
        "- **ppo/val/error**: Mean squared error between predicted values and returns.\n",
        "\n",
        "### **6. Loss Metrics**\n",
        "- **ppo/loss/policy**: Policy loss for PPO.\n",
        "- **ppo/loss/value**: Loss for the value function.\n",
        "- **ppo/loss/total**: Total loss, which is the sum of policy and value function losses.\n",
        "\n",
        "### **7. Token Metrics**\n",
        "- **tokens/queries_len_mean**: Average length of query tokens.\n",
        "- **tokens/queries_len_std**: Standard deviation of query token lengths.\n",
        "- **tokens/queries_dist**: Histogram distribution of query token lengths.\n",
        "- **tokens/responses_len_mean**: Average length of response tokens.\n",
        "- **tokens/responses_len_std**: Standard deviation of response token lengths.\n",
        "- **tokens/responses_dist**: Histogram distribution of response token lengths (note: `tokens/responses_len_dist` for consistency).\n",
        "\n",
        "### **8. Log Probability Metrics**\n",
        "- **objective/logprobs**: Histogram distribution of log probabilities of actions taken by the model.\n",
        "- **objective/ref_logprobs**: Histogram distribution of log probabilities of actions taken by the reference model.\n",
        "\n",
        "---\n",
        "\n",
        "## **Crucial Values**\n",
        "\n",
        "### **For Reward and Policy Monitoring:**\n",
        "- **env/reward_mean, env/reward_std, env/reward_dist**: Monitor the reward distribution from the reward model.\n",
        "- **ppo/mean_non_score_reward**: Mean negated KL penalty during training; indicates the deviation between reference and new policy.\n",
        "\n",
        "### **For Stability:**\n",
        "- **ppo/loss/value**: Spikes or NaNs may indicate issues with value function estimation.\n",
        "- **ppo/policy/ratio**: Ratio values should be around 1. High values (e.g., > 200) suggest overoptimization and potential instability.\n",
        "- **ppo/policy/clipfrac, ppo/policy/approxkl**: High values suggest that the policy is deviating significantly from the old policy, which can lead to instability.\n",
        "- **objective/kl**: Should remain positive; a low or negative value suggests the policy is drifting too far from the reference.\n",
        "- **objective/kl_coef**: Often increases before numerical instabilities arise.\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "f4VIg2veXURj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q datasets trl"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-08-14T02:56:50.778929Z",
          "iopub.execute_input": "2024-08-14T02:56:50.779669Z",
          "iopub.status.idle": "2024-08-14T02:57:04.037377Z",
          "shell.execute_reply.started": "2024-08-14T02:56:50.779636Z",
          "shell.execute_reply": "2024-08-14T02:57:04.036018Z"
        },
        "trusted": true,
        "id": "kkFx1G30XURl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from tqdm import tqdm\n",
        "import wandb\n",
        "\n",
        "from transformers import pipeline, AutoTokenizer\n",
        "from datasets import load_dataset\n",
        "\n",
        "from trl import PPOTrainer, PPOConfig, AutoModelForCausalLMWithValueHead,create_reference_model\n",
        "from trl.core import LengthSampler\n",
        "\n",
        "def build_dataset(config, dataset_name=\"imdb\", input_min_text_length=2, input_max_text_length=8):\n",
        "    # Build a dataset to be used for the training.\n",
        "    # It is a series of prompts (each with different length chosen randomly)\n",
        "    # We will use it to generate the responses and compute the rewards.\n",
        "    tokenizer = AutoTokenizer.from_pretrained(config.model_name)\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    # load the IMDB dataset\n",
        "    ds = load_dataset(dataset_name, split=\"train\")\n",
        "    ds = ds.rename_columns({\"text\": \"review\"})\n",
        "    # Only choose reviews with more than 200 tokens\n",
        "    ds = ds.filter(lambda x: len(x[\"review\"]) > 200, batched=False)\n",
        "\n",
        "    input_size = LengthSampler(input_min_text_length, input_max_text_length)\n",
        "\n",
        "#     def tokenize(sample):\n",
        "#         # From each review just keep the first `input_size` tokens, this represents the prompt used to generate the response\n",
        "#         sample[\"input_ids\"] = tokenizer.encode(sample[\"review\"])[: input_size()]\n",
        "#         sample[\"query\"] = tokenizer.decode(sample[\"input_ids\"])\n",
        "#         return sample\n",
        "\n",
        "    def tokenize(sample):\n",
        "        # Encode the review and generate the attention mask\n",
        "        input_ids = tokenizer.encode(sample[\"review\"])[:input_size()]\n",
        "        attention_mask = [1] * len(input_ids)  # Attention mask is 1 for all valid tokens\n",
        "\n",
        "        # Handle padding if needed (optional, but will be needed if sequence length varies)\n",
        "        if len(input_ids) < input_size():\n",
        "            padding_length = input_size() - len(input_ids)\n",
        "            input_ids += [tokenizer.pad_token_id] * padding_length\n",
        "            attention_mask += [0] * padding_length\n",
        "        # From each review just keep the first `input_size` tokens, this represents the prompt used to generate the response\n",
        "        sample[\"input_ids\"] = input_ids\n",
        "        sample[\"attention_mask\"] = attention_mask\n",
        "        sample[\"query\"] = tokenizer.decode(input_ids)\n",
        "        return sample\n",
        "\n",
        "    ds = ds.map(tokenize, batched=False)\n",
        "    ds.set_format(type=\"torch\")\n",
        "    return ds\n",
        "\n",
        "\n",
        "def collator(data):\n",
        "    return dict((key, [d[key] for d in data]) for key in data[0])\n",
        "\n",
        "\n",
        "config = PPOConfig(\n",
        "    model_name=\"lvwerra/gpt2-imdb\",\n",
        "    learning_rate=1.41e-5,\n",
        "    num_shared_layers=6,\n",
        "    log_with=\"wandb\",\n",
        ")\n",
        "\n",
        "wandb.init(\n",
        "        project=\"rlhf-load-dataset\",\n",
        "        config={\n",
        "            \"model_name\": config.model_name,\n",
        "            \"learning_rate\": config.learning_rate,\n",
        "            \"output_min_length\": 4,\n",
        "            \"output_max_length\": 16,\n",
        "            \"batch_size\": 16,  # Example batch size\n",
        "            \"epochs\": 10,  # Example number of epochs\n",
        "        }\n",
        "    )\n",
        "\n",
        "dataset = build_dataset(config)\n",
        "dataset"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-08-14T02:57:29.92749Z",
          "iopub.execute_input": "2024-08-14T02:57:29.928427Z",
          "iopub.status.idle": "2024-08-14T02:58:51.094125Z",
          "shell.execute_reply.started": "2024-08-14T02:57:29.928377Z",
          "shell.execute_reply": "2024-08-14T02:58:51.093096Z"
        },
        "trusted": true,
        "id": "1uvw3SbVXURn",
        "outputId": "c77d3447-4438-4297-bbcb-abe1df217764",
        "colab": {
          "referenced_widgets": [
            "c6415adc4e9646a589887f030910ec4d",
            "1e8263a951d34f5dba8d81ef71195278",
            "c5d1439782b44891a2de799b4a8e161b",
            "387040448ebd49cfadac03cb376bc060",
            "0c57b5ac308c4a7a9aaf867188ba37c3",
            "b3ef0e7ca1eb45e999509c28055dd9a2",
            "b10095b50df54e8489641eda0721d2b1",
            "93f0322921c944358adab71178075ebc",
            "e2e6c1db78a1409085d0f174eb6882ee",
            "e399eceb033d4ca794a6ab88d66865c0",
            "267de56d31ef427ca59157b23060e280",
            "a43f7e8015ca4bd7a243901b1a55f408",
            "da5e185a687f4f7185aa3a3820ee1a8a",
            "3142f2113f214922816a23397eab598e"
          ]
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "text": "2024-08-14 02:57:35.076107: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-08-14 02:57:35.076210: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-08-14 02:57:35.204791: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:",
          "output_type": "stream"
        },
        {
          "output_type": "stream",
          "name": "stdin",
          "text": "  ········································\n"
        },
        {
          "name": "stderr",
          "text": "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "wandb version 0.17.6 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "Tracking run with wandb version 0.17.4"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "Run data is saved locally in <code>/kaggle/working/wandb/run-20240814_025753-0eampfds</code>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "Syncing run <strong><a href='https://wandb.ai/tchoud8/rlhf-training/runs/0eampfds' target=\"_blank\">polished-salad-1</a></strong> to <a href='https://wandb.ai/tchoud8/rlhf-training' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": " View project at <a href='https://wandb.ai/tchoud8/rlhf-training' target=\"_blank\">https://wandb.ai/tchoud8/rlhf-training</a>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": " View run at <a href='https://wandb.ai/tchoud8/rlhf-training/runs/0eampfds' target=\"_blank\">https://wandb.ai/tchoud8/rlhf-training/runs/0eampfds</a>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "tokenizer_config.json:   0%|          | 0.00/17.0 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c6415adc4e9646a589887f030910ec4d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "config.json:   0%|          | 0.00/577 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1e8263a951d34f5dba8d81ef71195278"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c5d1439782b44891a2de799b4a8e161b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "387040448ebd49cfadac03cb376bc060"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "special_tokens_map.json:   0%|          | 0.00/90.0 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0c57b5ac308c4a7a9aaf867188ba37c3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Downloading readme:   0%|          | 0.00/7.81k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b3ef0e7ca1eb45e999509c28055dd9a2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Downloading data:   0%|          | 0.00/21.0M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b10095b50df54e8489641eda0721d2b1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Downloading data:   0%|          | 0.00/20.5M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "93f0322921c944358adab71178075ebc"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Downloading data:   0%|          | 0.00/42.0M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e2e6c1db78a1409085d0f174eb6882ee"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Generating train split:   0%|          | 0/25000 [00:00<?, ? examples/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e399eceb033d4ca794a6ab88d66865c0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Generating test split:   0%|          | 0/25000 [00:00<?, ? examples/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "267de56d31ef427ca59157b23060e280"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Generating unsupervised split:   0%|          | 0/50000 [00:00<?, ? examples/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a43f7e8015ca4bd7a243901b1a55f408"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Filter:   0%|          | 0/25000 [00:00<?, ? examples/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "da5e185a687f4f7185aa3a3820ee1a8a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Map:   0%|          | 0/24895 [00:00<?, ? examples/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3142f2113f214922816a23397eab598e"
            }
          },
          "metadata": {}
        },
        {
          "name": "stderr",
          "text": "Token indices sequence length is longer than the specified maximum sequence length for this model (1168 > 1024). Running this sequence through the model will result in indexing errors\n",
          "output_type": "stream"
        },
        {
          "execution_count": 5,
          "output_type": "execute_result",
          "data": {
            "text/plain": "Dataset({\n    features: ['review', 'label', 'input_ids', 'attention_mask', 'query'],\n    num_rows: 24895\n})"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This is the model we are going to fine-tune with PPO\n",
        "model = AutoModelForCausalLMWithValueHead.from_pretrained(config.model_name)\n",
        "# This is the reference model (frozen) for the KL divergence - Here for memory efficiency we're sharing 6 layers\n",
        "ref_model = create_reference_model(config.model_name,config.num_shared_layers)\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(config.model_name)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "ppo_trainer = PPOTrainer(config, model, ref_model, tokenizer, dataset=dataset, data_collator=collator)\n",
        "\n",
        "device = ppo_trainer.accelerator.device\n",
        "if ppo_trainer.accelerator.num_processes == 1:\n",
        "    device = 0 if torch.cuda.is_available() else \"cpu\"  # to avoid a `pipeline` bug\n",
        "\n",
        "# This is the reward model: a \"positive\" (e.g. a positive review) response will be given a high reward, a \"negative\" response will be given a low reward\n",
        "sentiment_pipe = pipeline(\"sentiment-analysis\", model=\"lvwerra/distilbert-imdb\", device=device)\n",
        "\n",
        "\n",
        "# Print some examples of sentiments generated by the reward model\n",
        "sent_kwargs = {\"return_all_scores\": True, \"function_to_apply\": \"none\", \"batch_size\": 16}\n",
        "text = \"this movie was really bad!!\"\n",
        "print(sentiment_pipe(text, **sent_kwargs))\n",
        "\n",
        "text = \"this movie was really good!!\"\n",
        "print(sentiment_pipe(text, **sent_kwargs)) # [{'label': 'NEGATIVE', 'score': -2.335047960281372}, {'label': 'POSITIVE', 'score': 2.557039737701416}]\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-08-14T02:58:59.98665Z",
          "iopub.execute_input": "2024-08-14T02:58:59.987055Z",
          "iopub.status.idle": "2024-08-14T02:59:32.456871Z",
          "shell.execute_reply.started": "2024-08-14T02:58:59.987024Z",
          "shell.execute_reply": "2024-08-14T02:59:32.455926Z"
        },
        "trusted": true,
        "id": "4MziuHzGXURp",
        "outputId": "35d6fd1a-29b0-493f-9430-c064c87c4a72",
        "colab": {
          "referenced_widgets": [
            "c776314556d84e8aacb69c485140d201",
            "",
            "c918e4430d4d4ce0acab613bdd5e4d66",
            "84cdb2232ff74f36b18efb23e7bb2c85",
            "5fc1114b4be146fabfab1080204402c7",
            "c3214a85081d452bb6a6510ab614bbae",
            "29410db8af614ea48e31e5c85e21505f",
            "b7d6f612650e4369b17f9cca71a740da"
          ]
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "pytorch_model.bin:   0%|          | 0.00/548M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c776314556d84e8aacb69c485140d201"
            }
          },
          "metadata": {}
        },
        {
          "name": "stderr",
          "text": "/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n  return self.fget.__get__(instance, owner)()\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "Finishing last run (ID:0eampfds) before initializing another..."
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "VBox(children=(Label(value='0.018 MB of 0.018 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": ""
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": " View run <strong style=\"color:#cdcd00\">polished-salad-1</strong> at: <a href='https://wandb.ai/tchoud8/rlhf-training/runs/0eampfds' target=\"_blank\">https://wandb.ai/tchoud8/rlhf-training/runs/0eampfds</a><br/> View project at: <a href='https://wandb.ai/tchoud8/rlhf-training' target=\"_blank\">https://wandb.ai/tchoud8/rlhf-training</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "Find logs at: <code>./wandb/run-20240814_025753-0eampfds/logs</code>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "Successfully finished last run (ID:0eampfds). Initializing new run:<br/>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "wandb version 0.17.6 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "Tracking run with wandb version 0.17.4"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "Run data is saved locally in <code>/kaggle/working/wandb/run-20240814_025907-92swyrdm</code>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "Syncing run <strong><a href='https://wandb.ai/tchoud8/trl/runs/92swyrdm' target=\"_blank\">olive-oath-1</a></strong> to <a href='https://wandb.ai/tchoud8/trl' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": " View project at <a href='https://wandb.ai/tchoud8/trl' target=\"_blank\">https://wandb.ai/tchoud8/trl</a>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": " View run at <a href='https://wandb.ai/tchoud8/trl/runs/92swyrdm' target=\"_blank\">https://wandb.ai/tchoud8/trl/runs/92swyrdm</a>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "config.json:   0%|          | 0.00/735 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c918e4430d4d4ce0acab613bdd5e4d66"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "pytorch_model.bin:   0%|          | 0.00/268M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "84cdb2232ff74f36b18efb23e7bb2c85"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "tokenizer_config.json:   0%|          | 0.00/333 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5fc1114b4be146fabfab1080204402c7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c3214a85081d452bb6a6510ab614bbae"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "29410db8af614ea48e31e5c85e21505f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b7d6f612650e4369b17f9cca71a740da"
            }
          },
          "metadata": {}
        },
        {
          "name": "stderr",
          "text": "/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "[[{'label': 'NEGATIVE', 'score': 2.3350484371185303}, {'label': 'POSITIVE', 'score': -2.726576328277588}]]\n[[{'label': 'NEGATIVE', 'score': -2.294790267944336}, {'label': 'POSITIVE', 'score': 2.557040214538574}]]\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "output_min_length = 4\n",
        "output_max_length = 16\n",
        "output_length_sampler = LengthSampler(output_min_length, output_max_length)\n",
        "\n",
        "# The configuration to generate responses (trajectories)\n",
        "response_generation_kwargs = {\n",
        "    \"min_length\": -1,\n",
        "    \"top_k\": 0.0,\n",
        "    \"top_p\": 1.0,\n",
        "    \"do_sample\": True,\n",
        "    \"pad_token_id\": tokenizer.eos_token_id,\n",
        "}\n",
        "\n",
        "\n",
        "def batch_process_pipeline(pipeline, texts, batch_size=16, **kwargs):\n",
        "    results = []\n",
        "    for i in range(0, len(texts), batch_size):\n",
        "        batch = texts[i:i + batch_size]\n",
        "        results.extend(pipeline(batch, **kwargs))\n",
        "    return results\n",
        "\n",
        "\n",
        "for epoch, batch in tqdm(enumerate(ppo_trainer.dataloader)):\n",
        "    query_tensors = batch[\"input_ids\"]\n",
        "\n",
        "    #### Phase 1: Get trajectories from the offline policy\n",
        "    # In this case we are only generating the responses, but not computing the log probabilities, which will be computed internally by the PPOTrainer.\n",
        "    response_tensors = []\n",
        "    for query in query_tensors:\n",
        "        gen_len = output_length_sampler()\n",
        "        response_generation_kwargs[\"max_new_tokens\"] = gen_len # Number of tokens to generate (chosen randomly)\n",
        "        response = ppo_trainer.generate(query, **response_generation_kwargs) # It returns the (query + response) tokens\n",
        "        response_tensors.append(response.squeeze()[-gen_len:]) # Only take the tokens corresponding to the generated response (remove the prompt/query from the beginning)\n",
        "    batch[\"response\"] = [tokenizer.decode(r.squeeze()) for r in response_tensors]\n",
        "\n",
        "    #### Phase 1: Compute rewards\n",
        "    # Join the query (prompt) + response (generated tokens)\n",
        "    texts = [q + r for q, r in zip(batch[\"query\"], batch[\"response\"])]\n",
        "    # Compute the reward for each of the texts (query + response)\n",
        "    # shape: A list of dictionaries with two keys: POSITIVE and NEGATIVE. We are interested in the POSITIVE score. This will be our reward.\n",
        "    pipe_outputs = batch_process_pipeline(sentiment_pipe, texts, **sent_kwargs)\n",
        "    # The reward for each text is the score (logit) corresponding to the POSITIVE class.\n",
        "    # shape: A list of scalars, one for each generated response.\n",
        "    # It means we assign the reward to the whole response (not to each token).\n",
        "    rewards = [torch.tensor(output[1][\"score\"]) for output in pipe_outputs]\n",
        "\n",
        "    #### Phase 1 + Phase 2: calculate the logprobs and then run the PPO update\n",
        "    stats = ppo_trainer.step(query_tensors, response_tensors, rewards)\n",
        "\n",
        "    # Log metrics to wandb\n",
        "    wandb.log({\n",
        "        \"epoch\": epoch,\n",
        "        \"average_reward\": torch.mean(torch.tensor(rewards)).item(),  # Example: log the average reward\n",
        "        \"logprobs\": stats.get(\"logprobs\"),  # Adjust according to your stats\n",
        "        # Add more stats or metrics you want to log\n",
        "    })\n",
        "\n",
        "    ppo_trainer.log_stats(stats, batch, rewards)\n",
        "\n",
        "# Finish wandb run\n",
        "wandb.finish()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-08-14T02:59:44.504833Z",
          "iopub.execute_input": "2024-08-14T02:59:44.505473Z",
          "iopub.status.idle": "2024-08-14T03:54:42.555619Z",
          "shell.execute_reply.started": "2024-08-14T02:59:44.505431Z",
          "shell.execute_reply": "2024-08-14T03:54:42.554657Z"
        },
        "trusted": true,
        "id": "GFktLBCDXURq",
        "outputId": "fce3a7bf-4ff7-4fbb-fbf9-c19703d96f42",
        "colab": {
          "referenced_widgets": [
            ""
          ]
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "text": "0it [00:00, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token.As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n1it [00:15, 15.88s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n194it [54:52, 16.97s/it]\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "VBox(children=(Label(value='4.918 MB of 4.918 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": ""
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>average_reward</td><td>▁▂▂▄▅▆▆▆▆▇▇▆▇▇▆▇▇▆▇▇▇▇▇▇▇███▇▇█████▇███▇</td></tr><tr><td>env/reward_mean</td><td>▁▂▂▄▅▆▆▆▆▇▇▆▇▇▆▇▇▆▇▇▇▇▇▇▇███▇▇█████▇███▇</td></tr><tr><td>env/reward_std</td><td>█▇▆▇▇▆▆▆▅▄▅▅▃▃▅▄▅▇▃▄▆▃▄▄▅▃▃▃▃▄▁▂▃▃▃▄▃▃▃▃</td></tr><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>objective/entropy</td><td>▆██▆▅▆▄▄▄▄▃▃▃▂▃▄▃▃▄▃▆▃▃▄▄▃▂▃▂▃▃▃▂▃▃▂▁▁▃▂</td></tr><tr><td>objective/kl</td><td>▁▁▂▃▄▅▅▅▅▅▆▅▆▆▅▅▆▆▆▆▇▆▇▇▆▇▇▇▆▇▇▆▆█▇▆▇█▇▇</td></tr><tr><td>objective/kl_coef</td><td>███▇▇▇▇▆▆▆▆▆▅▅▅▅▅▄▄▄▄▄▃▃▃▃▃▃▃▂▂▂▂▂▂▂▁▁▁▁</td></tr><tr><td>ppo/learning_rate</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>ppo/loss/policy</td><td>▁▄▄▆▆▆▆▅▆▅▆▇▆▆▆▆▅▄▇▆▅▇▇▅▆▇█▆▇▇█▇▆▇▇▆▆▇▇▅</td></tr><tr><td>ppo/loss/total</td><td>█▆▄▆▄▃▃▃▃▂▃▂▂▂▂▂▂▂▂▂▂▂▂▁▂▁▁▁▂▂▁▁▁▁▂▂▂▁▂▁</td></tr><tr><td>ppo/loss/value</td><td>█▆▄▅▄▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>ppo/mean_non_score_reward</td><td>█▇▇▅▄▃▂▂▂▂▁▂▂▂▃▂▁▂▂▂▁▂▂▁▂▂▁▁▂▁▂▃▂▁▂▃▂▁▂▂</td></tr><tr><td>ppo/mean_scores</td><td>▁▂▂▄▅▆▆▆▆▇▇▆▇▇▆▇▇▆▇▇▇▇▇▇▇███▇▇█████▇███▇</td></tr><tr><td>ppo/policy/advantages_mean</td><td>▁▇▇▄▇▅▃▂▇▄▄▅▅▃▆▄▇▇▅▄█▇█▄▄▄▆▅▆▅▃▄▃▄▆▆▄▆▃▅</td></tr><tr><td>ppo/policy/approxkl</td><td>▄▆▂▁▂▁▃▂▃▂▂▁▁▁▂█▃▃▁▂▂▂▁▂▂▂▁▂▂▂▁▁▁▁▂▂▁▁▁▂</td></tr><tr><td>ppo/policy/clipfrac</td><td>█▃▃▂▂▂▂▂▃▂▂▂▂▂▂▂▃▃▁▂▃▂▂▃▂▂▁▃▁▂▁▂▂▂▂▂▂▂▂▃</td></tr><tr><td>ppo/policy/entropy</td><td>▇▇█▇▆▃▅▄▃▄▄▃▃▂▂▃▃▃▂▁▃▂▂▃▃▂▂▁▂▂▁▂▁▁▁▂▁▂▁▁</td></tr><tr><td>ppo/policy/policykl</td><td>▇▇▅▅▆▄▅▄▄▆▄▅▅▄▅▆▇█▅▆▅▆▄▆▆▃▅▆▆▁▅▄▅▄▅▄▆▄▅▅</td></tr><tr><td>ppo/returns/mean</td><td>▁▂▃▄▅▆▆▆▆▇▇▇▇▇▇▇▆▆▇▇▇█▇▇▇███████████████</td></tr><tr><td>ppo/returns/var</td><td>█▇▄▇▆▅▅▅▄▄▄▄▂▂▄▄▄▆▃▃▅▂▄▃▅▂▂▃▂▃▁▁▃▃▂▃▂▂▂▂</td></tr><tr><td>ppo/std_scores</td><td>█▇▆▇▇▆▆▆▅▄▅▅▃▃▅▄▅▇▃▄▆▃▄▄▅▃▃▃▃▄▁▂▃▃▃▄▃▃▃▃</td></tr><tr><td>ppo/val/clipfrac</td><td>█▁▁▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>ppo/val/error</td><td>█▆▄▅▄▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>ppo/val/mean</td><td>▁▃▄▅▆▇▆▇▆▇▇▇▇▇▇▇▇▇▇▇▇▇██▇███████████▇███</td></tr><tr><td>ppo/val/var</td><td>█▅▃▄▂▃▂▂▂▂▃▂▁▂▂▂▃▄▃▂▂▂▃▃▃▂▂▂▁▂▁▁▂▂▁▂▃▂▁▂</td></tr><tr><td>ppo/val/var_explained</td><td>▁▂▃▄▅▆▆▆▆▆▆▆▆▆▇▇▇█▇▇▇▆███▇▇▇▇█▇▇██▇▇▇█▇█</td></tr><tr><td>ppo/val/vpred</td><td>▁▃▄▅▆▇▆▇▆▇▇▇▇▇▇▇▇▇▇▇▇▇██▇███████████▇███</td></tr><tr><td>time/ppo/calc_stats</td><td>▁▃▄▅▇▇▇▇██▇▇█▇▇▅▇▇▇▇█▇▇▇▇█▇▇▄█▇▇██▇▇▅▇▆▇</td></tr><tr><td>time/ppo/compute_advantages</td><td>▃▂▂▂▂▄▂▂▄▂▃▁▂▂▄▁▃▂▂▂▃▂▃▂▄▂▁▂▁█▂▁▂▂▂▂▂▁▁▃</td></tr><tr><td>time/ppo/compute_rewards</td><td>▁▃▁▁▁▂▂▂▃▂▂▁▂▂▂▂▂▂▁▂▂▂▁▂▂▂▂▁▁█▁▂▂▂▂▁▂▁▂▂</td></tr><tr><td>time/ppo/forward_pass</td><td>▁▂▃▅▇▆▆▆█▆▆▆▆▆▆▅▇▇▆▆▆▆▆▆▆▆▆▆▄▇▇▆▆▆▆▆▅▆▆▆</td></tr><tr><td>time/ppo/optimize_step</td><td>▁▃▄▆██▇██▇█▇▇█▇▆██▇██▇▇█████▅██████▇▅▇▇▇</td></tr><tr><td>time/ppo/total</td><td>▁▃▄▆▇▇▇▇█▇▇▇▇▇▇▅▇▇▇▇█▇▇▇▇▇▇█▅█▇▇█▇▇▇▅▇▇▇</td></tr><tr><td>tokens/queries_len_mean</td><td>▅▇▄▁▅▅▅▇▅▃▂▄▇▃▆▄▁▄▄▂▄▆▃▄▃▅▆▃▇▃▄▆▅▅▃▄▃█▄▅</td></tr><tr><td>tokens/queries_len_std</td><td>▄▃▄▅▆▆▄▂▆▄▅▃▁▆▄▅▃▄▅▅▆▆▅█▆▅▅▅▃▄▆▃▄▄▅▄▅▃▃▂</td></tr><tr><td>tokens/responses_len_mean</td><td>▄▅▅▄▃█▃▂▄▃▂▄▄▃▄▅▃▄▆▆█▄▇▅▇▅▅▅▁▅▆▄▃▆▅▃▃▂▆▄</td></tr><tr><td>tokens/responses_len_std</td><td>▅▆▅▄▄▅▆▃▆▄▅▁▅▄▅▄▆▇▇▆▅▄▄▆█▆▇▅▅▄▆▄█▅▆▄▁▃▇▆</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>average_reward</td><td>2.18062</td></tr><tr><td>env/reward_mean</td><td>2.18062</td></tr><tr><td>env/reward_std</td><td>1.045</td></tr><tr><td>epoch</td><td>193</td></tr><tr><td>objective/entropy</td><td>29.58419</td></tr><tr><td>objective/kl</td><td>5.24842</td></tr><tr><td>objective/kl_coef</td><td>0.12533</td></tr><tr><td>ppo/learning_rate</td><td>1e-05</td></tr><tr><td>ppo/loss/policy</td><td>-0.02309</td></tr><tr><td>ppo/loss/total</td><td>-0.00575</td></tr><tr><td>ppo/loss/value</td><td>0.17341</td></tr><tr><td>ppo/mean_non_score_reward</td><td>-0.07087</td></tr><tr><td>ppo/mean_scores</td><td>2.18062</td></tr><tr><td>ppo/policy/advantages_mean</td><td>-0.0</td></tr><tr><td>ppo/policy/approxkl</td><td>0.0019</td></tr><tr><td>ppo/policy/clipfrac</td><td>0.01978</td></tr><tr><td>ppo/policy/entropy</td><td>3.11062</td></tr><tr><td>ppo/policy/policykl</td><td>0.00086</td></tr><tr><td>ppo/returns/mean</td><td>1.98927</td></tr><tr><td>ppo/returns/var</td><td>0.72079</td></tr><tr><td>ppo/std_scores</td><td>1.045</td></tr><tr><td>ppo/val/clipfrac</td><td>0.0</td></tr><tr><td>ppo/val/error</td><td>0.34681</td></tr><tr><td>ppo/val/mean</td><td>2.07567</td></tr><tr><td>ppo/val/var</td><td>0.42964</td></tr><tr><td>ppo/val/var_explained</td><td>0.51885</td></tr><tr><td>ppo/val/vpred</td><td>2.05746</td></tr><tr><td>time/ppo/calc_stats</td><td>0.49052</td></tr><tr><td>time/ppo/compute_advantages</td><td>0.00237</td></tr><tr><td>time/ppo/compute_rewards</td><td>0.02021</td></tr><tr><td>time/ppo/forward_pass</td><td>0.47288</td></tr><tr><td>time/ppo/optimize_step</td><td>2.63382</td></tr><tr><td>time/ppo/total</td><td>3.61996</td></tr><tr><td>tokens/queries_len_mean</td><td>5.17969</td></tr><tr><td>tokens/queries_len_std</td><td>1.35991</td></tr><tr><td>tokens/responses_len_mean</td><td>9.28125</td></tr><tr><td>tokens/responses_len_std</td><td>3.5248</td></tr></table><br/></div></div>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": " View run <strong style=\"color:#cdcd00\">olive-oath-1</strong> at: <a href='https://wandb.ai/tchoud8/trl/runs/92swyrdm' target=\"_blank\">https://wandb.ai/tchoud8/trl/runs/92swyrdm</a><br/> View project at: <a href='https://wandb.ai/tchoud8/trl' target=\"_blank\">https://wandb.ai/tchoud8/trl</a><br/>Synced 5 W&B file(s), 194 media file(s), 194 artifact file(s) and 0 other file(s)"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "Find logs at: <code>./wandb/run-20240814_025907-92swyrdm/logs</code>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained(\"gpt2-imdb-pos-v2\", push_to_hub=True)\n",
        "tokenizer.save_pretrained(\"gpt2-imdb-pos-v2\", push_to_hub=True)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-08-14T04:01:33.4451Z",
          "iopub.execute_input": "2024-08-14T04:01:33.445865Z",
          "iopub.status.idle": "2024-08-14T04:01:51.704174Z",
          "shell.execute_reply.started": "2024-08-14T04:01:33.445833Z",
          "shell.execute_reply": "2024-08-14T04:01:51.703258Z"
        },
        "trusted": true,
        "id": "CXmvMxWNXURr",
        "outputId": "a8f5c7e9-ad76-4b77-ca4b-246b56079662",
        "colab": {
          "referenced_widgets": [
            "bd1db941013c43fab2a5c41d0b4d73c1"
          ]
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "model.safetensors:   0%|          | 0.00/498M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bd1db941013c43fab2a5c41d0b4d73c1"
            }
          },
          "metadata": {}
        },
        {
          "execution_count": 8,
          "output_type": "execute_result",
          "data": {
            "text/plain": "('gpt2-imdb-pos-v2/tokenizer_config.json',\n 'gpt2-imdb-pos-v2/special_tokens_map.json',\n 'gpt2-imdb-pos-v2/vocab.json',\n 'gpt2-imdb-pos-v2/merges.txt',\n 'gpt2-imdb-pos-v2/added_tokens.json',\n 'gpt2-imdb-pos-v2/tokenizer.json')"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "\n",
        "model_name_or_path = \"pritam3355/gpt2-imdb-pos-v2\"\n",
        "device = 0 if torch.cuda.is_available() else \"cpu\" # or \"cuda\" if you have a GPU\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name_or_path).to(device)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
        "\n",
        "inputs = tokenizer.encode(\"I'm not sure of the movie but I think it's\", return_tensors=\"pt\").to(device)\n",
        "outputs = model.generate(inputs)\n",
        "print(tokenizer.decode(outputs[0]))\n",
        "model"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-08-14T06:29:59.90513Z",
          "iopub.execute_input": "2024-08-14T06:29:59.905525Z",
          "iopub.status.idle": "2024-08-14T06:30:01.224385Z",
          "shell.execute_reply.started": "2024-08-14T06:29:59.905495Z",
          "shell.execute_reply": "2024-08-14T06:30:01.223223Z"
        },
        "trusted": true,
        "id": "LeKOvzzDXURs",
        "outputId": "ef30559d-352c-4a2d-e193-bccd3c1c56bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "text": "Some weights of the model checkpoint at pritam3355/gpt2-imdb-pos-v2 were not used when initializing GPT2LMHeadModel: ['v_head.summary.bias', 'v_head.summary.weight']\n- This IS expected if you are initializing GPT2LMHeadModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing GPT2LMHeadModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "I'm not sure of the movie but I think it's great. It's a great movie,\n",
          "output_type": "stream"
        },
        {
          "execution_count": 3,
          "output_type": "execute_result",
          "data": {
            "text/plain": "GPT2LMHeadModel(\n  (transformer): GPT2Model(\n    (wte): Embedding(50257, 768)\n    (wpe): Embedding(1024, 768)\n    (drop): Dropout(p=0.1, inplace=False)\n    (h): ModuleList(\n      (0-11): 12 x GPT2Block(\n        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (attn): GPT2SdpaAttention(\n          (c_attn): Conv1D()\n          (c_proj): Conv1D()\n          (attn_dropout): Dropout(p=0.1, inplace=False)\n          (resid_dropout): Dropout(p=0.1, inplace=False)\n        )\n        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (mlp): GPT2MLP(\n          (c_fc): Conv1D()\n          (c_proj): Conv1D()\n          (act): NewGELUActivation()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n  )\n  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n)"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#"
      ],
      "metadata": {
        "id": "NaU2C1R7XURy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}